{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hawk data starboard wing data API\n",
    "\n",
    "The aim of this notebook is to guide the user through the use of the API package `hawk` for interacting with the dataseries collected as part of the DTHIVE project ins 2022. In order to use this notebook, the `hawk` package is required. The package is freely available and can be installed with pip (requres python 3.9+):\n",
    "\n",
    "`pip install git+https://github.com/MDCHAMP/hawk-data`\n",
    "\n",
    "For more information about the test, head to the [INSERT LINK HERE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from hawk import SBW"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic usage\n",
    "\n",
    "The `hawk` package contains the `hawk.SBW` function for interacting with the data from the starboard wing test. During testing, data were collected in both the time and frequency domains as part of two test campaigns named after the measurement equipment. These are:\n",
    "\n",
    "- `LMS` (Frequency domain data)\n",
    "- `NI` (Time domain data)\n",
    "\n",
    "The `hawk.SBW` function provides a convenient wrapper for both of these test campaigns. To start exploring the data, simply call the function with a single argument of the path in which the data should be saved on disk.\n",
    "\n",
    "It is the intention of the authors that the hawk data should be entirely self-describing. In order to facilitate this, the hawk package implements two functions `describe` and `explore`. Let us see the effect of these functions now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"./hawk_data\"\n",
    "data = SBW(data_dir)\n",
    "data.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the test campaigns a great deal of data were collected. Within the `data` object we created above, the various test series and signals are organised like a file-tree structure. The `describe` method above returns information pertaining to where in the tree we are currently. The `explore` method provides a look at what is contained within the tree beneath us. Lets see the result of calling the `explore` method on `data` (the top of the tree)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.explore()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected this producdes a structured description of which data are available as well as data that is available but is not yet downloaded. \n",
    "\n",
    "Lets now try to access a test series that may or may not be downloaded in `data_dir` form the LMS campaign."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series = 'BR_AR' # i.e Burst-random amplitude-ramp \n",
    "rep = '01'\n",
    "test_series = data[\"LMS\"][series][rep]\n",
    "test_series.explore()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just by accessing the data in our code, the relevant files have been downloded and saved to disk in `data_dir`. If we were to access the data again, the downloaded copy wold be used automatically. \n",
    "\n",
    "Looking at the output of the `explore` function, we can see that there are a number of sensor addresses and for each one we are able to access three signals; the coherence, spectra and FRF. These signals are directly recovered form the 'LMS-box' measurement system.\n",
    "\n",
    "Lets now take a look at the output of the `describe` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_series.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is simply a python `dict` with all the details of the test setup, senor metadata and notes from the operators -- handy!\n",
    "\n",
    "Lets look now at one of the sensors. Sensors are accesed by their key. For more information on sensor locations please see [LINK TO PIC]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor = \"LLC-07\"  # Lower leading edge centre position 7 (wing tip)\n",
    "sensor_data = test_series[sensor]\n",
    "sensor_data.explore()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, there are three channels (`frf` is simply shorthand for `frequencyResponseFunction`). \n",
    "\n",
    "Lets (finally) plot the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal = \"frequencyResponseFunction\"\n",
    "frfs = sensor_data[signal]\n",
    "print(frfs.shape) # (spectralLines, Nrepeats)\n",
    "\n",
    "# Grab the frequency information from the xData group\n",
    "fs = data['LMS/xData/freq'] \n",
    "\n",
    "plt.figure()\n",
    "plt.semilogy(fs[:], np.abs(frfs[:]))\n",
    "# note that the units can be pulled directly from the data\n",
    "plt.xlabel(f\"{fs.attrs['measurement']} ({fs.attrs['units']})\") \n",
    "plt.ylabel(f\"{frfs.attrs['measurement']} ({frfs.attrs['units']})\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the metadata corresponding to the signals we are plotting is available in the `attrs` attribute of the dataset - how convenient!\n",
    "\n",
    "So far we have taken a roundabout may of accessing the FRFs. Instead, we could have just used the path to the data we were interested in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that we can access data directly from its path\n",
    "frfs = data['/LMS/BR_AR/01/LLC-01/frf']\n",
    "# or even\n",
    "frfs = data[f'/LMS/{series}/{rep}/{sensor}/{signal}']\n",
    "frfs.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the general form of the path:\n",
    "\n",
    "`/{campaign}/{series}/{rep}/{sensor}/{signal}`\n",
    "\n",
    "For more information on the possible values of these fields please see the data report available at [LINK TO DATA REPORT]. \n",
    "\n",
    "Lets now take a look at some of the time series data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = data['NI/xData/time']\n",
    "accs = data['NI/RPH_AR/01/LLC-01/acc']\n",
    "print(accs.shape) # (timePoints, Nrepeats)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(ts[:], accs[:])\n",
    "plt.xlabel(f\"{ts.attrs['measurement']} ({ts.attrs['units']})\") \n",
    "plt.ylabel(f\"{accs.attrs['measurement']} ({accs.attrs['units']})\")\n",
    "plt.show()\n",
    "\n",
    "# Can access a full description of the test setup\n",
    "accs.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In just a few lines of python code we have downloaded and plotted the dat alongside all of the relevant metadata. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced/production usage\n",
    "\n",
    "So far we have seen how to access and explore the data. Lets now cover some more realistic usage scenarios.\n",
    "\n",
    "Under the hood, all the data that is downloaded is in `.hdf5` format. This means that instead of using the `hawk` package you could instead use any off the shelf `.hdf5` viewing software. In fact, the `hawk` package is actually just a very simple wrapper for the `h5py` [link here] packcage that adds functionality for the `describe`, `explore` methods and manages automatic downloads -- neat!\n",
    "\n",
    "Because the data is stored in `.hdf5` format, it is only read from disk when it is needed by the python program. This means that python keeps open (several) file objects while interacting with the data. To prevent corruption and other bugs, files in python should always be closed when they are not bieng used. Thankfully, the `SBW` function support python context managers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with SBW(data_dir) as data:\n",
    "\n",
    "    frfs = data['/LMS/BR_AR/01/LLC-01/frf']\n",
    "    frfs_numpy = np.array(data['/LMS/BR_AR/01/LLC-01/frf'])\n",
    "\n",
    "try:\n",
    "    data['/LMS/BR_AR/01/LLC-01/frf'].shape # This fails because the file is now closed\n",
    "except ValueError:\n",
    "    pass\n",
    "\n",
    "frfs.shape # this is ok because the variable frfs is still referenced BUT internal references may be broken\n",
    "frfs_numpy.shape # this will always be ok and should be considered best practice"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that once outside the context manager, the `data` object can no longer be interacted with. In the above, the variable `frf` maintains a link to the file object and so can still be accessed, however, internal links may be broken and so this behavouir should also be avoided.\n",
    "\n",
    "You may have noticed in the plotting code above that the data were sliced into the `plot` function. This is because although the underlying `h5py.Dataset` implements some of the same functionality as `np.ndarray`, it cannot be considered a drop-in replacement.  \n",
    "\n",
    "For best practice in production code, the required data should be cast to numpy arrays inside the context manager. There are several ways to achive this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with SBW(data_dir) as data:\n",
    "\n",
    "    frf1 = data['/LMS/BR_AR/01/LLC-01/frf']\n",
    "    frf2 = np.array(data['/LMS/BR_AR/01/LLC-01/frf']) # return np.ndarray\n",
    "    frf3 = data['/LMS/BR_AR/01/LLC-01/frf'][:] # also returns np.ndarray\n",
    "    frf4 = data['/LMS/BR_AR/01/LLC-01/frf'][:100] # also returns np.ndarray (simple sliceing only)\n",
    "\n",
    "print(type(frf1))\n",
    "print(type(frf2))\n",
    "print(type(frf3))\n",
    "print(type(frf4))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset structure\n",
    "\n",
    "In order to avoid downloading all the data every time, the dataset has been divided into a number of independent files, each one correspondiong to one of the test series repeats. Overall there are 71 test series. \n",
    "\n",
    "The `hawk` package relies on a single 'header' `.hdf5` file for accessing all of the data simultaneously without loading it all in to memory (or even having all of the data on disk). This works thanks to the `ExternalLink` feature of the `.hdf5` spec, more details of which can be found at [LINK HERE]."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
